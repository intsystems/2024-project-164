@inproceedings{Bengio2012RepresentationLA,
  title={Representation Learning: A Review and New Perspectives},
  author={Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2012},
  volume={35},
  pages={1798-1828},
  url={https://api.semanticscholar.org/CorpusID:393948}
}

@inproceedings{Pang2022UnsupervisedVR,
  title={Unsupervised Visual Representation Learning by Synchronous Momentum Grouping},
  author={Bo Pang and Yifan Zhang and Yaoyi Li and Jia Cai and Cewu Lu},
  booktitle={European Conference on Computer Vision},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:250490993}
}

@inproceedings{GroupFace,
author = {Kim, Yonghyun and Park, Wonpyo and Roh, Myung-Cheol and Shin, Jongju},
year = {2020},
month = {06},
pages = {5620-5629},
title = {GroupFace: Learning Latent Groups and Constructing Group-Based Representations for Face Recognition},
doi = {10.1109/CVPR42600.2020.00566}
}

@inproceedings{Elezi2019TheGL,
  title={The Group Loss for Deep Metric Learning},
  author={Ismail Elezi and Sebastiano Vascon and Alessandro Torcinovich and Marcello Pelillo and Laura Leal-Taix{\'e}},
  booktitle={European Conference on Computer Vision},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:208527171}
}

@inproceedings{doi:10.1126/science.aab3050,
author = {Brenden M. Lake  and Ruslan Salakhutdinov  and Joshua B. Tenenbaum },
title = {Human-level concept learning through probabilistic program induction},
journal = {Science},
volume = {350},
number = {6266},
pages = {1332-1338},
year = {2015},
doi = {10.1126/science.aab3050},
URL = {https://www.science.org/doi/abs/10.1126/science.aab3050}}


@inproceedings{tan2020efficientnet,
      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2020},
      eprint={1905.11946},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Bootstrapping,
author = {Wan, Shaohua and Chen, Zhijun and Zhang, Tao and Zhang, Bo and Wong, Kong-kat},
year = {2016},
month = {08},
pages = {},
title = {Bootstrapping Face Detection with Hard Negative Examples}
}

@inproceedings{10.1145/3514221.3517848,
author = {Isenko, Alexander and Mayer, Ruben and Jedele, Jeffrey and Jacobsen, Hans-Arno},
title = {Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517848},
doi = {10.1145/3514221.3517848},
abstract = {Preprocessing pipelines in deep learning aim to provide sufficient data throughput to keep the training processes busy. Maximizing resource utilization is becoming more challenging as the throughput of training processes increases with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and advanced parallelization techniques that yield better scalability. At the same time, the amount of training data needed in order to train increasingly complex models is growing. As a consequence of this development, data preprocessing and provisioning are becoming a severe bottleneck in end-to-end deep learning pipelines.In this paper, we provide an in-depth analysis of data preprocessing pipelines from four different machine learning domains. We introduce a new perspective on efficiently preparing datasets for end-to-end deep learning pipelines and extract individual trade-offs to optimize throughput, preprocessing time, and storage consumption. Additionally, we provide an open-source profiling library that can automatically decide on a suitable preprocessing strategy to maximize throughput. By applying our generated insights to real-world use-cases, we obtain an increased throughput of 3x to 13x compared to an untuned system while keeping the pipeline functionally identical. These findings show the enormous potential of data pipeline tuning.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1825â€“1839},
numpages = {15},
keywords = {preprocessing, machine learning, deep learning, datasets, data processing},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{Li2019PreprocessingMA,
  title={Preprocessing Methods and Pipelines of Data Mining: An Overview},
  author={Cancheng Li},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.08510},
  url={https://api.semanticscholar.org/CorpusID:195218459}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@inproceedings{baghaei2022deep,
  title={Deep representation learning: Fundamentals, perspectives, applications, and open challenges},
  author={Baghaei, Kourosh T and Payandeh, Amirreza and Fayyazsanavi, Pooya and Rahimi, Shahram and Chen, Zhiqian and Ramezani, Somayeh Bakhtiari},
  journal={arXiv preprint arXiv:2211.14732},
  year={2022}
}

@inproceedings{SiameseNetworks,
author = {Heidari, Mohsen and Fouladi, Kazim},
year = {2020},
month = {02},
pages = {1-4},
title = {Using Siamese Networks with Transfer Learning for Face Recognition on Small-Samples Datasets},
doi = {10.1109/MVIP49855.2020.9116915}
}

@inproceedings{TripletLoss,
author = {Balntas, Vassileios and Riba, Edgar and Ponsa, Daniel and Mikolajczyk, Krystian},
year = {2016},
month = {01},
pages = {119.1-119.11},
title = {Learning local feature descriptors with triplets and shallow convolutional neural networks},
doi = {10.5244/C.30.119}
}